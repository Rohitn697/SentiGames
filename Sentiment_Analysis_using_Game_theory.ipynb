{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ioxESQkECwMM"
      },
      "source": [
        "## SentiGames: Game Theoretic approach to Sentiment Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Importing the wordnet and the sentiment lexicons"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "I9t4UtYDCwMS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpNkKglECwMT",
        "outputId": "ee12726f-f8cd-4783-f4f3-d16553b70b97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing tensorflow"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "cmG17CSWCwMV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/download\n",
            "  16384/Unknown - 0s 0us/step"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-cd6a346aef87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtrain_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mremove_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unsup'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './aclImdb/train'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)\n",
        "\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)\n",
        "\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "thr0Ta2iCwMV",
        "outputId": "c857a31c-ec62-46ca-e967-d893811ed69a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yrz6Md3d6zEQ",
        "outputId": "ea6f41e5-f0b7-4939-ac8b-54bbd5a1e473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "stop_words.remove('not')\n",
        "stop_words.remove('no')\n",
        "stop_words.remove('nor')\n",
        "stop_words.remove('don\\'t')\n",
        "stop_words.remove('doesn\\'t')\n",
        "stop_words.remove('didn\\'t')\n",
        "stop_words.remove('isn\\'t')\n",
        "stop_words.remove('aren\\'t')\n",
        "stop_words.remove('wasn\\'t')\n",
        "stop_words.remove('weren\\'t')\n",
        "stop_words.remove('won\\'t')\n",
        "stop_words.remove('wouldn\\'t')\n",
        "stop_words.remove('shouldn\\'t')\n",
        "stop_words.remove('couldn\\'t')\n",
        "stop_words.remove('mightn\\'t')\n",
        "stop_words.remove('mustn\\'t')\n",
        "\n",
        "def remove_stop_words(sentence_words):\n",
        "\t\treturn [word for word in sentence_words if word not in stop_words]\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    lowercase = tf.strings.lower(sentence)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "    text = tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation),\n",
        "                                  '')\n",
        "    text = text.numpy().decode('utf-8')\n",
        "    sentence_words = nltk.word_tokenize(text)\n",
        "    sentence_words = remove_stop_words(sentence_words)\n",
        "    for word in sentence_words:\n",
        "        word = lemmatizer.lemmatize(word)\n",
        "    return sentence_words\n",
        "\n",
        "def extract_senti_synsets(words):\n",
        "  senti_synsets = []\n",
        "  sense_count = []\n",
        "  new_words = []\n",
        "\n",
        "  for word in words:\n",
        "    word_synset=[]\n",
        "    for synset in swn.senti_synsets(word):\n",
        "      word_synset.append(synset)\n",
        "\n",
        "    unique_word_synsets = []\n",
        "    for senti_synset in word_synset:\n",
        "      if senti_synset not in unique_word_synsets:\n",
        "        if senti_synset.synset.name().split('.')[1] != 'n':\n",
        "          unique_word_synsets.append(senti_synset)\n",
        "\n",
        "    if not len(unique_word_synsets) <= 5:   # Number of unique senses = 5 (assumption)\n",
        "        unique_word_synsets = unique_word_synsets[0:5]\n",
        "\n",
        "    for senti_synset in unique_word_synsets:\n",
        "        senti_synsets.append(senti_synset)\n",
        "\n",
        "    if len(unique_word_synsets) > 0:\n",
        "      sense_count.append(len(unique_word_synsets))\n",
        "      new_words.append(word)\n",
        "\n",
        "  return senti_synsets, new_words, sense_count\n",
        "\n",
        "def cosine_similarity(senti_synset1, senti_synset2):\n",
        "  dot = senti_synset1.pos_score()*senti_synset2.pos_score() + senti_synset1.neg_score()*senti_synset2.neg_score() + senti_synset1.obj_score()*senti_synset2.obj_score()\n",
        "  norm1 = np.sqrt(senti_synset1.pos_score()**2 + senti_synset1.neg_score()**2 + senti_synset1.obj_score()**2)\n",
        "  norm2 = np.sqrt(senti_synset2.pos_score()**2 + senti_synset2.neg_score()**2 + senti_synset2.obj_score()**2)\n",
        "  if norm1 == 0 or norm2 == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return dot/(norm1*norm2)\n",
        "\n",
        "def replicator_dynamic(words, payoff_matrix, num_iterations, senti_count, senti_start_index):\n",
        "    for i in range(num_iterations):\n",
        "        for player in range(len(words)):\n",
        "            player_payoff = 0\n",
        "            strategy_payoff = np.zeros((senti_count[player], 1), dtype=float)\n",
        "            sense_preference_player = np.array(strategy_space[player:player + 1,\n",
        "                                               senti_start_index[player]:senti_start_index[player] + senti_count[\n",
        "                                                   player]])\n",
        "            for neighbour in range(len(words)):\n",
        "                if neighbour == player:\n",
        "                    continue\n",
        "                temp_matrix = np.array(payoff_matrix[\n",
        "                                         senti_start_index[player]:senti_start_index[player] + senti_count[player],\n",
        "                                         senti_start_index[neighbour]:senti_start_index[neighbour] + senti_count[\n",
        "                                             neighbour]], dtype=float)\n",
        "                sense_preference_neighbour = np.array(strategy_space[neighbour:neighbour + 1,\n",
        "                                                      senti_start_index[neighbour]:senti_start_index[neighbour] +\n",
        "                                                                                   senti_count[neighbour]])\n",
        "                sense_preference_neighbour = sense_preference_neighbour.transpose()\n",
        "                current_payoff = np.dot(temp_matrix, sense_preference_neighbour)\n",
        "                strategy_payoff = np.add(current_payoff, strategy_payoff)\n",
        "                player_payoff += np.dot(sense_preference_player, current_payoff)\n",
        "            updation_values = np.ones(strategy_payoff.shape)\n",
        "            if not player_payoff == 0:\n",
        "                updation_values = np.divide(strategy_payoff, player_payoff)\n",
        "            for j in range(0, senti_count[player]):\n",
        "                strategy_space[player][senti_start_index[player] + j] = strategy_space[player][\n",
        "                                                                            senti_start_index[player] + j] * \\\n",
        "                                                                        updation_values[j]\n",
        "\n",
        "def generate_sentiment_similarity_matrix(senti_synsets):\n",
        "  similarity_matrix = np.zeros((len(senti_synsets), len(senti_synsets)), dtype=float)\n",
        "  for i in range(len(senti_synsets)):\n",
        "    for j in range(i, len(senti_synsets)):\n",
        "      similarity = cosine_similarity(senti_synsets[i], senti_synsets[j])\n",
        "\n",
        "      if similarity == None:\n",
        "        similarity = 0\n",
        "      similarity_matrix[i][j] = similarity\n",
        "      similarity_matrix[j][i] = similarity\n",
        "  return similarity_matrix"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "mEKpgJVuCwMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Defining a sample sentence to work upon and make it lower case\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "XD7aayiWCwMb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['really', 'liked', 'movie', 'avengers', 'really', 'awesome']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "example = \"I really liked the movie Avengers as it was really awesome\"\n",
        "removed_stop_words = preprocess_sentence(example)\n",
        "\n",
        "removed_stop_words"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg0D9YWsCwMb",
        "outputId": "abcbca9b-cb5a-4e32-fb24-b3d3a027ec4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Importing glove which is comprised of 6 Billion words with 50 dimentions\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "XXsGtD_XCwMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF1Jvvr0KIm9",
        "outputId": "d6fd8672-7ae3-49c4-ff72-955e7db10172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"/content/drive/MyDrive/glove.6B.50d.txt\", encoding='utf8')\n",
        "\n",
        "embedding_index = dict()\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    emb = np.array(values[1:], dtype ='float')\n",
        "    embedding_index[word] = emb\n",
        "    "
      ],
      "metadata": {
        "id": "Zz34_csUKpLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_index['awesome'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ty0vPoJLI4B",
        "outputId": "495c1c0d-109d-4929-a1e7-4a2fb0aea785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.35848  -0.1155    0.11371   0.46814   0.7495   -0.61523   0.47639\n",
            "  0.090754  0.3689    0.50331  -0.22467   0.234    -0.64901   0.055667\n",
            "  0.30192  -0.13536   0.93473   0.88677  -0.70756  -0.48408  -0.90625\n",
            "  0.62314  -0.18793  -0.5102    1.2565   -0.28897  -1.2819    0.30284\n",
            "  1.0423   -0.54885   1.0054    0.62053   0.31879  -0.060822 -0.24919\n",
            "  0.5019    0.41171   0.13648  -0.49815  -0.59822  -0.16876  -0.26096\n",
            " -0.53283   0.20083  -0.19095  -0.028693  0.090843 -0.11063  -0.040858\n",
            "  0.88439 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Here each word is represented as a vector of 50 numerical values"
      ],
      "metadata": {
        "id": "khhCkEdxMS4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Defining a function to get_embedding_output"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "4XmY_j9fCwMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It will take a text as an input and then convert each word into a vector of 50 elements."
      ],
      "metadata": {
        "id": "GLjfNXI-MaoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_embedding_output(X):\n",
        "    maxLen = 10\n",
        "    embedding_output = np.zeros((len(X), maxLen, 50))\n",
        "\n",
        "    for ix in range(np.asarray(X).shape[0]):\n",
        "        my_example = X[ix].split()\n",
        "       \n",
        "        for ij in range(len(my_example)):\n",
        "            if (embedding_index.get(my_example[ij].lower()) is not None) and (ij < maxLen):\n",
        "                embedding_output[ix][ij] = embedding_index[my_example[ij].lower()]\n",
        "\n",
        "    return embedding_output"
      ],
      "metadata": {
        "id": "Uke5obiKNlXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_embed = get_embedding_output(removed_stop_words)\n",
        "x_train_embed[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an7qvRdUN1xQ",
        "outputId": "289f6f5e-e080-4a9a-bffa-efcef81d3162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.6675e-03, -1.6376e-01, -9.2648e-02, -3.3466e-01,  7.3972e-01,\n",
              "        -2.3523e-01, -3.4941e-01,  1.9102e-01, -4.2223e-01,  5.8440e-01,\n",
              "        -2.7604e-01,  4.6605e-01, -9.7154e-01,  3.5971e-02,  8.9279e-01,\n",
              "         5.0195e-01,  8.9409e-01,  3.5050e-01,  1.2178e-01, -9.1063e-01,\n",
              "        -6.7188e-01,  8.4035e-01,  3.1734e-01,  3.3727e-01,  1.3483e+00,\n",
              "        -1.9291e+00, -1.1992e+00,  6.0348e-01,  1.2938e+00, -9.2512e-01,\n",
              "         3.2757e+00,  7.5342e-01,  6.4755e-02, -3.1481e-01, -3.7328e-01,\n",
              "        -2.3711e-01, -2.5322e-01,  5.5946e-01,  2.6690e-01, -6.6446e-01,\n",
              "        -4.2612e-01, -5.1564e-02, -1.8357e-02,  4.1999e-01,  3.5430e-01,\n",
              "         2.6320e-01,  5.1319e-02, -2.4906e-02, -6.9572e-02,  1.1343e+00],\n",
              "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "removed_stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj-0MlpgQSD2",
        "outputId": "78fc6994-942d-4620-edba-bc111eb35732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'really'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 4, 9, 13]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "all_senti_synsets, removed_noun_words, sense_count  = extract_senti_synsets(removed_stop_words)\n",
        "\n",
        "sense_start_index = [0]\n",
        "for i in range(len(sense_count)-1):\n",
        "  sense_start_index.append(sense_start_index[i]+sense_count[i])\n",
        "\n",
        "sense_start_index\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOFgYX1RCwMd",
        "outputId": "6e3bafb1-157f-4df0-ed7c-f976bba7b7bd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Forming strategy space\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "PP4ccUbWCwMd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "array([[1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n        0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.  , 0.  , 0.  , 0.  , 0.  ,\n        0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.25, 0.25, 0.25, 0.25, 0.  ,\n        0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.2 ,\n        0.2 , 0.2 , 0.2 , 0.2 ]])"
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "strategy_space = np.zeros((len(removed_noun_words), len(all_senti_synsets)), dtype=float)\n",
        "start_index=0\n",
        "for i in range(len(removed_noun_words)):\n",
        "  for j in range(start_index, start_index+sense_count[i]):\n",
        "    strategy_space[i][j] = 1.0/sense_count[i]\n",
        "\n",
        "  start_index += sense_count[i]\n",
        "\n",
        "strategy_space"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "PlhPKhM8CwMe",
        "outputId": "8f42ba8e-61ed-46a8-c08d-a0e4524f75b5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Defining Cosine sentiment similarity function for senti_synsets"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "5DBKkNkECwMe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "e9ow5_XkCwMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Calculation of Payoff Matrix"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "U411SPWTCwMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "array([[1.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.70710678,\n        1.        , 0.70710678, 1.        , 1.        , 1.        ],\n       [0.        , 1.        , 1.        , 1.        , 1.        ,\n        1.        , 1.        , 1.        , 1.        , 0.70710678,\n        0.        , 0.70710678, 0.        , 0.        , 0.        ],\n       [0.        , 1.        , 1.        , 1.        , 1.        ,\n        1.        , 1.        , 1.        , 1.        , 0.70710678,\n        0.        , 0.70710678, 0.        , 0.        , 0.        ],\n       [0.        , 1.        , 1.        , 1.        , 1.        ,\n        1.        , 1.        , 1.        , 1.        , 0.70710678,\n        0.        , 0.70710678, 0.        , 0.        , 0.        ],\n       [0.        , 1.        , 1.        , 1.        , 1.        ,\n        1.        , 1.        , 1.        , 1.        , 0.70710678,\n        0.        , 0.70710678, 0.        , 0.        , 0.        ],\n       [0.        , 1.        , 1.        , 1.        , 1.        ,\n        1.        , 1.        , 1.        , 1.        , 0.70710678,\n        0.        , 0.70710678, 0.        , 0.        , 0.        ],\n       [0.        , 1.        , 1.        , 1.        , 1.        ,\n        1.        , 1.        , 1.        , 1.        , 0.70710678,\n        0.        , 0.70710678, 0.        , 0.        , 0.        ],\n       [0.        , 1.        , 1.        , 1.        , 1.        ,\n        1.        , 1.        , 1.        , 1.        , 0.70710678,\n        0.        , 0.70710678, 0.        , 0.        , 0.        ],\n       [0.        , 1.        , 1.        , 1.        , 1.        ,\n        1.        , 1.        , 1.        , 1.        , 0.70710678,\n        0.        , 0.70710678, 0.        , 0.        , 0.        ],\n       [0.70710678, 0.70710678, 0.70710678, 0.70710678, 0.70710678,\n        0.70710678, 0.70710678, 0.70710678, 0.70710678, 1.        ,\n        0.70710678, 1.        , 0.70710678, 0.70710678, 0.70710678],\n       [1.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.70710678,\n        1.        , 0.70710678, 1.        , 1.        , 1.        ],\n       [0.70710678, 0.70710678, 0.70710678, 0.70710678, 0.70710678,\n        0.70710678, 0.70710678, 0.70710678, 0.70710678, 1.        ,\n        0.70710678, 1.        , 0.70710678, 0.70710678, 0.70710678],\n       [1.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.70710678,\n        1.        , 0.70710678, 1.        , 1.        , 1.        ],\n       [1.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.70710678,\n        1.        , 0.70710678, 1.        , 1.        , 1.        ],\n       [1.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.70710678,\n        1.        , 0.70710678, 1.        , 1.        , 1.        ]])"
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "sentiment_similarity_matrix = generate_sentiment_similarity_matrix(all_senti_synsets)\n",
        "sentiment_similarity_matrix"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7xvb4xTzCwMf",
        "outputId": "ca5493b1-c4f7-4c12-e295-b6720fe1a55f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. defining Replicator Dynamic as a function"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "vw3HU-zBCwMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Strategy Space Updated\n",
            "[[1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 2.00000000e-01 2.00000000e-01 2.00000000e-01\n",
            "  2.00000000e-01 2.00000000e-01 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 2.53298603e-04 2.53298603e-04\n",
            "  2.53298603e-04 9.99240104e-01 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 4.81971695e-04 9.98072113e-01\n",
            "  4.81971695e-04 4.81971695e-04 4.81971695e-04]]\n"
          ]
        }
      ],
      "source": [
        "number_of_iterations = 20\n",
        "\n",
        "# replicator_dynamic(words, payoff_matrix, num_iterations, senti_count, senti_start_index)\n",
        "\n",
        "replicator_dynamic(removed_noun_words, sentiment_similarity_matrix, number_of_iterations, sense_count,\n",
        "                   sense_start_index)\n",
        "print (\"\\nStrategy Space Updated\")\n",
        "print (strategy_space)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "OvTKJAbSCwMf",
        "outputId": "d87925e9-2daa-4630-c994-fb1269856c98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Getting relevant senti senses"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "hPr57c0aCwMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "not: <not.r.01: PosScore=0.0 NegScore=0.625>\n",
            "like: <wish.v.02: PosScore=0.125 NegScore=0.0>\n",
            "really: <very.r.01: PosScore=0.25 NegScore=0.25>\n",
            "bad: <bad.s.02: PosScore=0.25 NegScore=0.25>\n"
          ]
        }
      ],
      "source": [
        "for word in range(len(removed_noun_words)):\n",
        "  print(removed_noun_words[word] + \": \", end=\"\")\n",
        "  max_value = 0\n",
        "  required_synset = None\n",
        "  for synset in range(len(all_senti_synsets)):\n",
        "    if strategy_space[word][synset] > max_value:\n",
        "        max_value = strategy_space[word][synset]\n",
        "        required_synset = all_senti_synsets[synset]\n",
        "  print(required_synset)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JLnZszGdCwMg",
        "outputId": "1573ba24-a0c0-48e4-a02e-8276a56d79e8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing document 1\n",
            "processing document 2\n",
            "processing document 3\n",
            "processing document 4\n",
            "processing document 5\n",
            "processing document 6\n",
            "processing document 7\n",
            "processing document 8\n",
            "processing document 9\n",
            "processing document 10\n",
            "processing document 11\n",
            "processing document 12\n",
            "processing document 13\n",
            "processing document 14\n",
            "processing document 15\n",
            "processing document 16\n",
            "processing document 17\n",
            "processing document 18\n",
            "processing document 19\n",
            "processing document 20\n",
            "processing document 21\n",
            "processing document 22\n",
            "processing document 23\n",
            "processing document 24\n",
            "processing document 25\n",
            "processing document 26\n",
            "processing document 27\n",
            "processing document 28\n",
            "processing document 29\n",
            "processing document 30\n",
            "processing document 31\n",
            "processing document 32\n",
            "processing document 33\n",
            "processing document 34\n",
            "processing document 35\n",
            "processing document 36\n",
            "processing document 37\n",
            "processing document 38\n",
            "processing document 39\n",
            "processing document 40\n",
            "processing document 41\n",
            "processing document 42\n",
            "processing document 43\n",
            "processing document 44\n",
            "processing document 45\n",
            "processing document 46\n",
            "processing document 47\n",
            "processing document 48\n",
            "processing document 49\n",
            "processing document 50\n",
            "processing document 51\n",
            "processing document 52\n",
            "processing document 53\n",
            "processing document 54\n",
            "processing document 55\n",
            "processing document 56\n",
            "processing document 57\n",
            "processing document 58\n",
            "processing document 59\n",
            "processing document 60\n",
            "processing document 61\n",
            "processing document 62\n",
            "processing document 63\n",
            "processing document 64\n",
            "processing document 65\n",
            "processing document 66\n",
            "processing document 67\n",
            "processing document 68\n",
            "processing document 69\n",
            "processing document 70\n",
            "processing document 71\n",
            "processing document 72\n",
            "processing document 73\n",
            "processing document 74\n",
            "processing document 75\n",
            "processing document 76\n",
            "processing document 77\n",
            "processing document 78\n",
            "processing document 79\n",
            "processing document 80\n",
            "processing document 81\n",
            "processing document 82\n",
            "processing document 83\n",
            "processing document 84\n",
            "processing document 85\n",
            "processing document 86\n",
            "processing document 87\n",
            "processing document 88\n",
            "processing document 89\n",
            "processing document 90\n",
            "processing document 91\n",
            "processing document 92\n",
            "processing document 93\n",
            "processing document 94\n",
            "processing document 95\n",
            "processing document 96\n",
            "processing document 97\n",
            "processing document 98\n",
            "processing document 99\n",
            "processing document 100\n",
            "processing document 101\n",
            "processing document 102\n",
            "processing document 103\n",
            "processing document 104\n",
            "processing document 105\n",
            "processing document 106\n",
            "processing document 107\n",
            "processing document 108\n",
            "processing document 109\n",
            "processing document 110\n",
            "processing document 111\n",
            "processing document 112\n",
            "processing document 113\n",
            "processing document 114\n",
            "processing document 115\n",
            "processing document 116\n",
            "processing document 117\n",
            "processing document 118\n",
            "processing document 119\n",
            "processing document 120\n",
            "processing document 121\n",
            "processing document 122\n",
            "processing document 123\n",
            "processing document 124\n",
            "processing document 125\n",
            "processing document 126\n",
            "processing document 127\n",
            "processing document 128\n",
            "processing document 129\n",
            "processing document 130\n",
            "processing document 131\n",
            "processing document 132\n",
            "processing document 133\n",
            "processing document 134\n",
            "processing document 135\n",
            "processing document 136\n",
            "processing document 137\n",
            "processing document 138\n",
            "processing document 139\n",
            "processing document 140\n",
            "processing document 141\n",
            "processing document 142\n",
            "processing document 143\n",
            "processing document 144\n",
            "processing document 145\n",
            "processing document 146\n",
            "processing document 147\n",
            "processing document 148\n",
            "processing document 149\n",
            "processing document 150\n",
            "processing document 151\n",
            "processing document 152\n",
            "processing document 153\n",
            "processing document 154\n",
            "processing document 155\n",
            "processing document 156\n",
            "processing document 157\n",
            "processing document 158\n",
            "processing document 159\n",
            "processing document 160\n",
            "processing document 161\n",
            "processing document 162\n",
            "processing document 163\n",
            "processing document 164\n",
            "processing document 165\n",
            "processing document 166\n",
            "processing document 167\n",
            "processing document 168\n",
            "processing document 169\n",
            "processing document 170\n",
            "processing document 171\n",
            "processing document 172\n",
            "processing document 173\n",
            "processing document 174\n",
            "processing document 175\n",
            "processing document 176\n",
            "processing document 177\n",
            "processing document 178\n",
            "processing document 179\n",
            "processing document 180\n",
            "processing document 181\n",
            "processing document 182\n",
            "processing document 183\n",
            "processing document 184\n",
            "processing document 185\n",
            "processing document 186\n",
            "processing document 187\n",
            "processing document 188\n",
            "processing document 189\n",
            "processing document 190\n",
            "processing document 191\n",
            "processing document 192\n",
            "processing document 193\n",
            "processing document 194\n",
            "processing document 195\n",
            "processing document 196\n",
            "processing document 197\n",
            "processing document 198\n",
            "processing document 199\n",
            "processing document 200\n",
            "processing document 201\n",
            "processing document 202\n",
            "processing document 203\n",
            "processing document 204\n",
            "processing document 205\n",
            "processing document 206\n",
            "processing document 207\n",
            "processing document 208\n",
            "processing document 209\n",
            "processing document 210\n",
            "processing document 211\n",
            "processing document 212\n",
            "processing document 213\n",
            "processing document 214\n",
            "processing document 215\n",
            "processing document 216\n",
            "processing document 217\n",
            "processing document 218\n",
            "processing document 219\n",
            "processing document 220\n",
            "processing document 221\n",
            "processing document 222\n",
            "processing document 223\n",
            "processing document 224\n",
            "processing document 225\n",
            "processing document 226\n",
            "processing document 227\n",
            "processing document 228\n",
            "processing document 229\n",
            "processing document 230\n",
            "processing document 231\n",
            "processing document 232\n",
            "processing document 233\n",
            "processing document 234\n",
            "processing document 235\n",
            "processing document 236\n",
            "processing document 237\n",
            "processing document 238\n",
            "processing document 239\n",
            "processing document 240\n",
            "processing document 241\n",
            "processing document 242\n",
            "processing document 243\n",
            "processing document 244\n",
            "processing document 245\n",
            "processing document 246\n",
            "processing document 247\n",
            "processing document 248\n",
            "processing document 249\n",
            "processing document 250\n",
            "processing document 251\n",
            "processing document 252\n",
            "processing document 253\n",
            "processing document 254\n",
            "processing document 255\n",
            "processing document 256\n",
            "processing document 257\n",
            "processing document 258\n",
            "processing document 259\n",
            "processing document 260\n",
            "processing document 261\n",
            "processing document 262\n",
            "processing document 263\n",
            "processing document 264\n",
            "processing document 265\n",
            "processing document 266\n",
            "processing document 267\n",
            "processing document 268\n",
            "processing document 269\n",
            "processing document 270\n",
            "processing document 271\n",
            "processing document 272\n",
            "processing document 273\n",
            "processing document 274\n",
            "processing document 275\n",
            "processing document 276\n",
            "processing document 277\n",
            "processing document 278\n",
            "processing document 279\n",
            "processing document 280\n",
            "processing document 281\n",
            "processing document 282\n",
            "processing document 283\n",
            "processing document 284\n",
            "processing document 285\n",
            "processing document 286\n",
            "processing document 287\n",
            "processing document 288\n",
            "processing document 289\n",
            "processing document 290\n",
            "processing document 291\n",
            "processing document 292\n",
            "processing document 293\n",
            "processing document 294\n",
            "processing document 295\n",
            "processing document 296\n",
            "processing document 297\n",
            "processing document 298\n",
            "processing document 299\n",
            "processing document 300\n",
            "processing document 301\n",
            "processing document 302\n",
            "processing document 303\n",
            "processing document 304\n",
            "processing document 305\n",
            "processing document 306\n",
            "processing document 307\n",
            "processing document 308\n",
            "processing document 309\n",
            "processing document 310\n",
            "processing document 311\n",
            "processing document 312\n",
            "processing document 313\n",
            "processing document 314\n",
            "processing document 315\n",
            "processing document 316\n",
            "processing document 317\n",
            "processing document 318\n",
            "processing document 319\n",
            "processing document 320\n",
            "processing document 321\n",
            "processing document 322\n",
            "processing document 323\n",
            "processing document 324\n",
            "processing document 325\n",
            "processing document 326\n",
            "processing document 327\n",
            "processing document 328\n",
            "processing document 329\n",
            "processing document 330\n",
            "processing document 331\n",
            "processing document 332\n",
            "processing document 333\n",
            "processing document 334\n",
            "processing document 335\n",
            "processing document 336\n",
            "processing document 337\n",
            "processing document 338\n",
            "processing document 339\n",
            "processing document 340\n",
            "processing document 341\n",
            "processing document 342\n",
            "processing document 343\n",
            "processing document 344\n",
            "processing document 345\n",
            "processing document 346\n",
            "processing document 347\n",
            "processing document 348\n",
            "processing document 349\n",
            "processing document 350\n",
            "processing document 351\n",
            "processing document 352\n",
            "processing document 353\n",
            "processing document 354\n",
            "processing document 355\n",
            "processing document 356\n",
            "processing document 357\n",
            "processing document 358\n",
            "processing document 359\n",
            "processing document 360\n",
            "processing document 361\n",
            "processing document 362\n",
            "processing document 363\n",
            "processing document 364\n",
            "processing document 365\n",
            "processing document 366\n",
            "processing document 367\n",
            "processing document 368\n",
            "processing document 369\n",
            "processing document 370\n",
            "processing document 371\n",
            "processing document 372\n",
            "processing document 373\n",
            "processing document 374\n",
            "processing document 375\n",
            "processing document 376\n",
            "processing document 377\n",
            "processing document 378\n",
            "processing document 379\n",
            "processing document 380\n",
            "processing document 381\n",
            "processing document 382\n",
            "processing document 383\n",
            "processing document 384\n",
            "processing document 385\n",
            "processing document 386\n",
            "processing document 387\n",
            "processing document 388\n",
            "processing document 389\n",
            "processing document 390\n",
            "processing document 391\n",
            "processing document 392\n",
            "processing document 393\n",
            "processing document 394\n",
            "processing document 395\n",
            "processing document 396\n",
            "processing document 397\n",
            "processing document 398\n",
            "processing document 399\n",
            "processing document 400\n",
            "processing document 401\n",
            "processing document 402\n",
            "processing document 403\n",
            "processing document 404\n",
            "processing document 405\n",
            "processing document 406\n",
            "processing document 407\n",
            "processing document 408\n",
            "processing document 409\n",
            "processing document 410\n",
            "processing document 411\n",
            "processing document 412\n",
            "processing document 413\n",
            "processing document 414\n",
            "processing document 415\n",
            "processing document 416\n",
            "processing document 417\n",
            "processing document 418\n",
            "processing document 419\n",
            "processing document 420\n",
            "processing document 421\n",
            "processing document 422\n",
            "processing document 423\n",
            "processing document 424\n",
            "processing document 425\n",
            "processing document 426\n",
            "processing document 427\n",
            "processing document 428\n",
            "processing document 429\n",
            "processing document 430\n",
            "processing document 431\n",
            "processing document 432\n",
            "processing document 433\n",
            "processing document 434\n",
            "processing document 435\n",
            "processing document 436\n",
            "processing document 437\n",
            "processing document 438\n",
            "processing document 439\n",
            "processing document 440\n",
            "processing document 441\n",
            "processing document 442\n",
            "processing document 443\n",
            "processing document 444\n",
            "processing document 445\n",
            "processing document 446\n",
            "processing document 447\n",
            "processing document 448\n",
            "processing document 449\n",
            "processing document 450\n",
            "processing document 451\n",
            "processing document 452\n",
            "processing document 453\n",
            "processing document 454\n",
            "processing document 455\n",
            "processing document 456\n",
            "processing document 457\n",
            "processing document 458\n",
            "processing document 459\n",
            "processing document 460\n",
            "processing document 461\n",
            "processing document 462\n",
            "processing document 463\n",
            "processing document 464\n",
            "processing document 465\n",
            "processing document 466\n",
            "processing document 467\n",
            "processing document 468\n",
            "processing document 469\n",
            "processing document 470\n",
            "processing document 471\n",
            "processing document 472\n",
            "processing document 473\n",
            "processing document 474\n",
            "processing document 475\n",
            "processing document 476\n",
            "processing document 477\n",
            "processing document 478\n",
            "processing document 479\n",
            "processing document 480\n",
            "processing document 481\n",
            "processing document 482\n",
            "processing document 483\n",
            "processing document 484\n",
            "processing document 485\n",
            "processing document 486\n",
            "processing document 487\n",
            "processing document 488\n",
            "processing document 489\n",
            "processing document 490\n",
            "processing document 491\n",
            "processing document 492\n",
            "processing document 493\n",
            "processing document 494\n",
            "processing document 495\n",
            "processing document 496\n",
            "processing document 497\n",
            "processing document 498\n",
            "processing document 499\n",
            "processing document 500\n",
            "processing document 501\n",
            "processing document 502\n",
            "processing document 503\n",
            "processing document 504\n",
            "processing document 505\n",
            "processing document 506\n",
            "processing document 507\n",
            "processing document 508\n",
            "processing document 509\n",
            "processing document 510\n",
            "processing document 511\n",
            "processing document 512\n",
            "processing document 513\n",
            "processing document 514\n",
            "processing document 515\n",
            "processing document 516\n",
            "processing document 517\n",
            "processing document 518\n",
            "processing document 519\n",
            "processing document 520\n",
            "processing document 521\n",
            "processing document 522\n",
            "processing document 523\n",
            "processing document 524\n",
            "processing document 525\n",
            "processing document 526\n",
            "processing document 527\n",
            "processing document 528\n",
            "processing document 529\n",
            "processing document 530\n",
            "processing document 531\n",
            "processing document 532\n",
            "processing document 533\n",
            "processing document 534\n",
            "processing document 535\n",
            "processing document 536\n",
            "processing document 537\n",
            "processing document 538\n",
            "processing document 539\n",
            "processing document 540\n",
            "processing document 541\n",
            "processing document 542\n",
            "processing document 543\n",
            "processing document 544\n",
            "processing document 545\n",
            "processing document 546\n",
            "processing document 547\n",
            "processing document 548\n",
            "processing document 549\n",
            "processing document 550\n",
            "processing document 551\n",
            "processing document 552\n",
            "processing document 553\n",
            "processing document 554\n",
            "processing document 555\n",
            "processing document 556\n",
            "processing document 557\n",
            "processing document 558\n",
            "processing document 559\n",
            "processing document 560\n",
            "processing document 561\n",
            "processing document 562\n",
            "processing document 563\n",
            "processing document 564\n",
            "processing document 565\n",
            "processing document 566\n",
            "processing document 567\n",
            "processing document 568\n",
            "processing document 569\n",
            "processing document 570\n",
            "processing document 571\n",
            "processing document 572\n",
            "processing document 573\n",
            "processing document 574\n",
            "processing document 575\n",
            "processing document 576\n",
            "processing document 577\n",
            "processing document 578\n",
            "processing document 579\n",
            "processing document 580\n",
            "processing document 581\n",
            "processing document 582\n",
            "processing document 583\n",
            "processing document 584\n",
            "processing document 585\n",
            "processing document 586\n",
            "processing document 587\n",
            "processing document 588\n",
            "processing document 589\n",
            "processing document 590\n",
            "processing document 591\n",
            "processing document 592\n",
            "processing document 593\n",
            "processing document 594\n",
            "processing document 595\n",
            "processing document 596\n",
            "processing document 597\n",
            "processing document 598\n",
            "processing document 599\n",
            "processing document 600\n",
            "processing document 601\n",
            "processing document 602\n",
            "processing document 603\n",
            "processing document 604\n",
            "processing document 605\n",
            "processing document 606\n",
            "processing document 607\n",
            "processing document 608\n",
            "processing document 609\n",
            "processing document 610\n",
            "processing document 611\n",
            "processing document 612\n",
            "processing document 613\n",
            "processing document 614\n",
            "processing document 615\n",
            "processing document 616\n",
            "processing document 617\n",
            "processing document 618\n",
            "processing document 619\n",
            "processing document 620\n",
            "processing document 621\n",
            "processing document 622\n",
            "processing document 623\n",
            "processing document 624\n",
            "processing document 625\n",
            "processing document 626\n",
            "processing document 627\n",
            "processing document 628\n",
            "processing document 629\n",
            "processing document 630\n",
            "processing document 631\n",
            "processing document 632\n",
            "processing document 633\n",
            "processing document 634\n",
            "processing document 635\n",
            "processing document 636\n",
            "processing document 637\n",
            "processing document 638\n",
            "processing document 639\n",
            "processing document 640\n"
          ]
        }
      ],
      "source": [
        "results= []\n",
        "expected_results = []\n",
        "x = 1\n",
        "for text_batch, label_batch in raw_train_ds.take(20):\n",
        "    for idx in range(text_batch.shape[0]):\n",
        "        print('processing document %d' % x)\n",
        "        x += 1\n",
        "        words = preprocess_sentence(text_batch[idx])\n",
        "        senti_synsets, new_words, sense_count = extract_senti_synsets(words)\n",
        "        sense_start_index = [0]\n",
        "        for i in range(len(sense_count) - 1):\n",
        "            sense_start_index.append(sense_start_index[i] + sense_count[i])\n",
        "        similarity_matrix = generate_sentiment_similarity_matrix(senti_synsets)\n",
        "        strategy_space = np.zeros((len(new_words), len(senti_synsets)), dtype=float)\n",
        "        start_index = 0\n",
        "        for i in range(len(new_words)):\n",
        "            for j in range(start_index, start_index + sense_count[i]):\n",
        "                strategy_space[i][j] = 1.0 / sense_count[i]\n",
        "\n",
        "            start_index += sense_count[i]\n",
        "        replicator_dynamic(new_words, similarity_matrix, 5, sense_count, sense_start_index)\n",
        "        pos_score = 0\n",
        "        neg_score = 0\n",
        "        for word in range(len(new_words)):\n",
        "            max_value = 0\n",
        "            required_synset = None\n",
        "            for synset in range(len(senti_synsets)):\n",
        "                if strategy_space[word][synset] > max_value:\n",
        "                    max_value = strategy_space[word][synset]\n",
        "                    required_synset = senti_synsets[synset]\n",
        "                if required_synset is not None:\n",
        "                  pos_score += required_synset.pos_score()\n",
        "                  neg_score += required_synset.neg_score()\n",
        "\n",
        "        if pos_score > neg_score:\n",
        "            results.append(1)\n",
        "        else:\n",
        "            results.append(0)\n",
        "        expected_results.append(label_batch.numpy()[idx])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "NZrTeDP5CwMg",
        "outputId": "22aa3e28-8bf2-47b2-d128-c8d184cf5d1c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[165 178]\n",
            " [ 63 234]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.48      0.58       343\n",
            "           1       0.57      0.79      0.66       297\n",
            "\n",
            "    accuracy                           0.62       640\n",
            "   macro avg       0.65      0.63      0.62       640\n",
            "weighted avg       0.65      0.62      0.62       640\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.confusion_matrix(expected_results, results, labels=[0, 1]))\n",
        "print(metrics.classification_report(expected_results, results, labels=[0, 1]))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "pbF0iN0MCwMh",
        "outputId": "23e41c38-c0b8-4669-98e2-bb8ca871ff1c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Rs4D3YIeCwMh"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}